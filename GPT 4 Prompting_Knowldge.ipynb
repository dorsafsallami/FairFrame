{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19153,
     "status": "ok",
     "timestamp": 1709740054169,
     "user": {
      "displayName": "dorsaf sallemi",
      "userId": "11081128056738234729"
     },
     "user_tz": 300
    },
    "id": "vWqdfZORK0Tp",
    "outputId": "c4a2fab6-9eea-4f3f-83c9-55f969b045ca"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3142,
     "status": "ok",
     "timestamp": 1709740057309,
     "user": {
      "displayName": "dorsaf sallemi",
      "userId": "11081128056738234729"
     },
     "user_tz": 300
    },
    "id": "B_NB9TgzLH-Z",
    "outputId": "2e641a55-37de-40bd-8cff-c00c038e71bc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "PATH = \"/content/drive/MyDrive/Fairness/data/MBIC.xlsx\"\n",
    "df = pd.read_excel(PATH)\n",
    "\n",
    "columns_to_remove = ['Unnamed: 0', 'news_link', 'outlet', 'topic', 'type',\n",
    "                     'group_id', 'num_sent', 'Label_opinion', 'article',\n",
    "                     'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13']\n",
    "\n",
    "\n",
    "df = df.drop(columns=columns_to_remove, errors='ignore')  \n",
    "df = df[df['Label_bias'] == 'Biased']\n",
    "df = df.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6400,
     "status": "ok",
     "timestamp": 1709740063677,
     "user": {
      "displayName": "dorsaf sallemi",
      "userId": "11081128056738234729"
     },
     "user_tz": 300
    },
    "id": "mNB9_VtJGMEX",
    "outputId": "c44e1412-6538-4b5c-d913-fed4123c4b39"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1709740064194,
     "user": {
      "displayName": "dorsaf sallemi",
      "userId": "11081128056738234729"
     },
     "user_tz": 300
    },
    "id": "3gvCJiOUbTmE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1709740065174,
     "user": {
      "displayName": "dorsaf sallemi",
      "userId": "11081128056738234729"
     },
     "user_tz": 300
    },
    "id": "aHXgVVPkbkbl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = ''  # Replace  with your actual OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1153,
     "status": "ok",
     "timestamp": 1709740066325,
     "user": {
      "displayName": "dorsaf sallemi",
      "userId": "11081128056738234729"
     },
     "user_tz": 300
    },
    "id": "dTvG-LldTYye"
   },
   "outputs": [],
   "source": [
    "df_bias = pd.read_excel('/content/drive/MyDrive/Fairness/data/bias_word_lexicon.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1709740066325,
     "user": {
      "displayName": "dorsaf sallemi",
      "userId": "11081128056738234729"
     },
     "user_tz": 300
    },
    "id": "DF_woWYHTrHT"
   },
   "outputs": [],
   "source": [
    "biased_words = df_bias['slavishness'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqcVavDXMb-Y"
   },
   "source": [
    "# Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 498483,
     "status": "ok",
     "timestamp": 1709741593694,
     "user": {
      "displayName": "dorsaf sallemi",
      "userId": "11081128056738234729"
     },
     "user_tz": 300
    },
    "id": "RQ84-_jhMkDg"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def debias_text_conversation(text, biased_words):\n",
    "\n",
    "    \n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"As an AI, you are aware of various biases that can appear in language. These biases can be related to gender, race, age, or cultural background. It is important to address these biases to ensure neutrality and inclusiveness. Some words that may reflect bias include: \" + \", \".join(biased_words) + \".\" + \"You are an assistant trained to identify and remove biases from text. Make sure the text is neutral, inclusive, and respects all individuals regardless of their gender, race, age, or cultural background.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Please debias the following text: {text}\"}\n",
    "    ]\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\", \n",
    "        messages=conversation\n",
    "    )\n",
    "\n",
    "    \n",
    "    if response.choices and response.choices[0].message:\n",
    "        \n",
    "        debiased_text = response.choices[0].message.content\n",
    "    else:\n",
    "        debiased_text = \"Debiasing failed\"  \n",
    "\n",
    "    return debiased_text\n",
    "\n",
    "\n",
    "\n",
    "df['debias_text'] = df['sentence'].apply(lambda x: debias_text_conversation(x, biased_words))\n",
    "\n",
    "\n",
    "df.to_csv('/content/drive/MyDrive/Fairness/data/Knowledge-zeroShot-Debiasing.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR65Y4tiM2Fe"
   },
   "source": [
    "# 2-shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 526744,
     "status": "ok",
     "timestamp": 1709742155439,
     "user": {
      "displayName": "dorsaf sallemi",
      "userId": "11081128056738234729"
     },
     "user_tz": 300
    },
    "id": "x_X7pCFuM8Jf"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def debias_text_conversation(text, biased_words):\n",
    "\n",
    "   \n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"As an AI, you are aware of various biases that can appear in language. These biases can be related to gender, race, age, or cultural background. It is important to address these biases to ensure neutrality and inclusiveness. Some words that may reflect bias include: \" + \", \".join(biased_words) + \".\" + \"You are an assistant trained to identify and remove biases from text. Make sure the text is neutral, inclusive, and respects all individuals regardless of their gender, race, age, or cultural background. For example, if the text says 'Billie Eilish issues apology for mouthing an anti-Asian derogatory term in a resurfaced video', you should debias it to Billie Eilish issues apology for using an anti-Asian remark in a resurfaced video'. If the text says 'If he is elected, he would be the first person of color in the White House.', debias it to 'lf he is elected, he would be the first African-American in the White House.'.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Please debias the following text: {text}\"}\n",
    "    ]\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "\n",
    "  \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",  \n",
    "        messages=conversation\n",
    "    )\n",
    "\n",
    "    \n",
    "    if response.choices and response.choices[0].message:\n",
    "        \n",
    "        debiased_text = response.choices[0].message.content\n",
    "    else:\n",
    "        debiased_text = \"Debiasing failed\"  \n",
    "\n",
    "    return debiased_text\n",
    "\n",
    "\n",
    "\n",
    "df['debias_text'] = df['sentence'].apply(lambda x: debias_text_conversation(x, biased_words))\n",
    "\n",
    "\n",
    "df.to_csv('/content/drive/MyDrive/Fairness/data/Knowledge-TwoShots-Debiasing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCFOS1WtB9O5"
   },
   "source": [
    "# 4-shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 511052,
     "status": "ok",
     "timestamp": 1709742919529,
     "user": {
      "displayName": "dorsaf sallemi",
      "userId": "11081128056738234729"
     },
     "user_tz": 300
    },
    "id": "QnPheeSNCBwx"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def debias_text_conversation(text, biased_words):\n",
    "\n",
    "    \n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"As an AI, you are aware of various biases that can appear in language. These biases can be related to gender, race, age, or cultural background. It is important to address these biases to ensure neutrality and inclusiveness. Some words that may reflect bias include: \" + \", \".join(biased_words) + \".\" + \"You are an assistant trained to identify and remove biases from text. Make sure the text is neutral, inclusive, and respects all individuals regardless of their gender, race, age, or cultural background. For example, if the text says 'Billie Eilish issues apology for mouthing an anti-Asian derogatory term in a resurfaced video', you should debias it to Billie Eilish issues apology for using an anti-Asian remark in a resurfaced video'. If the text says 'If he is elected, he would be the first person of color in the White House.', debias it to 'lf he is elected, he would be the first African-American in the White House.’, if the text says ‘He has had the physical handicap since he was 5 years old.', you should debias it to ‘He has had the physical impairment since he was 5 years old.'. If the text says’ There are many elderly people in our town.', debias it to’ There are many senior citizens (or seniors) in our town.'.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Please debias the following text: {text}\"}\n",
    "    ]\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "\n",
    "   \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",  \n",
    "        messages=conversation\n",
    "    )\n",
    "\n",
    "   \n",
    "    if response.choices and response.choices[0].message:\n",
    "        \n",
    "        debiased_text = response.choices[0].message.content\n",
    "    else:\n",
    "        debiased_text = \"Debiasing failed\"  \n",
    "\n",
    "    return debiased_text\n",
    "\n",
    "\n",
    "\n",
    "df['debias_text'] = df['sentence'].apply(lambda x: debias_text_conversation(x, biased_words))\n",
    "\n",
    "\n",
    "df.to_csv('/content/drive/MyDrive/Fairness/data/Knowledge-FourShots-Debiasing.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNIaVFW7PH/vqV2I+RyWRVf",
   "gpuType": "V100",
   "mount_file_id": "1SY-KICtMc9hZieP1xaF0ZMmCUSNYvgme",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
